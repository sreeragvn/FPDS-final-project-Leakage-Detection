{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to do/doubts/discussions\n",
    "# how to save subclassing api\n",
    "# hyperparameter tuning\n",
    "# how to save the best model among all the epochs\n",
    "# should we do augmentation of validation data ?\n",
    "# should we shuffle repeat, prefetch etc ?\n",
    "# All values are already between -1 to +1. should we do scaling on top of this ?\n",
    "# if we are adding scaling of data, how to ensure that when it is being tested by brauer our code would give out rescaled data\n",
    "# test cases are not sucessful - its not overfitting\n",
    "# incase of early stopping - what metric is to be monitored - val loss or val mse ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "# from sklearn import preprocessing\n",
    "# Make numpy values easier to read.\n",
    "# np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices(\"gpu\")\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# np.random.seed(101)\n",
    "# tf.random.set_seed(101)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data of data\n",
    "\n",
    "leakage_train_100 = pd.read_csv(\"leakage_dataset_train_100.csv\")\n",
    "leakage_train_1000 = pd.read_csv(\"leakage_dataset_train_1000.csv\")\n",
    "leakage_val_1000 = pd.read_csv(\"leakage_dataset_validation_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data):\n",
    "\n",
    "    train_ds = train_data\n",
    "    val_ds = leakage_val_1000\n",
    "\n",
    "    train_ds = train_ds.sample(frac=1)\n",
    "    val_ds = val_ds.sample(frac=1)\n",
    "\n",
    "    X_train = train_ds.iloc[:,2:].to_numpy()\n",
    "    Y_train = train_ds.iloc[:,:2]\n",
    "\n",
    "    X_validation = val_ds.iloc[:,2:].to_numpy()\n",
    "    Y_validation = val_ds.iloc[:,:2]\n",
    "\n",
    "    Y_train = Y_train.to_numpy()\n",
    "    Y_validation = Y_validation.to_numpy()\n",
    "    return X_train, Y_train, X_validation, Y_validation\n",
    "\n",
    "X_train, Y_train, X_validation, Y_validation = load_data(leakage_train_1000)\n",
    "\n",
    "\n",
    "#test dataset\n",
    "X_test = np.array([[0.25, 0.25, 0.25,0.25], \n",
    "                    [0.35, 0.15, 0.25,0.25], \n",
    "                    [0.25, 0.25, 0.15,0.35],\n",
    "                    [0.15, 0.35, 0.25,0.25],\n",
    "                    [0.25, 0.25, 0.35,0.15],\n",
    "                    [0.4, 0.1, 0.25,0.25],\n",
    "                    [0.1, 0.4, 0.25,0.25],\n",
    "                    [0.42, 0.08, 0.25,0.25],\n",
    "                    [0.25, 0.25, 0.4,0.1],\n",
    "                    [0.25, 0.25, 0.1,0.4]])\n",
    "                    \n",
    "# scX = preprocessing.StandardScaler()\n",
    "# stY = preprocessing.StandardScaler()\n",
    "\n",
    "# def data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY):\n",
    "#     X_train = scX.fit_transform(X_train)\n",
    "#     X_validation = scX.transform(X_validation)\n",
    "\n",
    "#     Y_train = stY.fit_transform(Y_train)\n",
    "#     Y_validation = stY.transform(Y_validation)\n",
    "\n",
    "#     return X_train, X_validation, Y_train, Y_validation\n",
    "\n",
    "# X_train, X_validation, Y_train, Y_validation = data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_validation = tf.convert_to_tensor(X_validation)\n",
    "Y_train = tf.convert_to_tensor(Y_train)\n",
    "Y_validation = tf.convert_to_tensor(Y_validation)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "\n",
    "# num_rows, num_cols = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000\n",
    "# steps_per_epoch = sum(train_occurences) / batch_size\n",
    "starter_learning_rate = 1e-1\n",
    "end_learning_rate = 1e-8\n",
    "decay_steps = epochs * 3\n",
    "scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "    decay_steps= decay_steps,\n",
    "    end_learning_rate= end_learning_rate,\n",
    "    power=1)\n",
    "kernel_regularizer=tf.keras.regularizers.L1L2(0.01)\n",
    "callbacks=[keras.callbacks.EarlyStopping(monitor='val_mean_squared_error',patience=20)]\n",
    "initializer=tf.keras.initializers.HeUniform\n",
    "\n",
    "def learning_curves(history)   :\n",
    "    sns.set_style('darkgrid', {'axes.facecolor': '.9'})\n",
    "    sns.set_context('notebook')\n",
    "\n",
    "    # your code\n",
    "    ### Learning curves\n",
    "    history_frame = pd.DataFrame(history.history)\n",
    "    history_frame.plot(figsize=(8, 5))\n",
    "    plt.show()\n",
    "\n",
    "def prediction_accuracy(predictions, Y_validation): \n",
    "    predictions = predictions.transpose()\n",
    "    Y_validation = tf.transpose(Y_validation)\n",
    "    y1 = predictions[0]\n",
    "    y2 = predictions[1]\n",
    "    y1_validation = Y_validation[0]\n",
    "    y2_validation = Y_validation[1]\n",
    "    fig, axs = plt.subplots(2)\n",
    "    # print(y1_validation.shape, y1.shape)\n",
    "    # print(y2_validation.shape, y2.shape)\n",
    "    # fig.suptitle('')\n",
    "    axs[0].scatter(y1_validation, y1)\n",
    "    axs[0].set_title('y1')\n",
    "    axs[1].scatter(y2_validation, y2)\n",
    "    axs[1].set_title('y2')\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='true value', ylabel='predicted value')\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    print(\"rmse of y1: \", mean_squared_error(y1_validation, y1, squared=False))\n",
    "    print(\"rmse of y2: \", mean_squared_error(y2_validation, y2, squared=False))\n",
    "\n",
    "# def dataloading(data):\n",
    "#     data = data.repeat()\n",
    "#     data = data.shuffle(buffer_size=1024, seed=0)\n",
    "#     data = data.batch(batch_size=batch_size)\n",
    "#     data = data.prefetch(buffer_size=1)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Augmentation\n",
    "# Requires cleaning up\n",
    "\n",
    "def rotation_matrix(angle):\n",
    "    theta = np.radians(angle)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c, -s), (s, c)))\n",
    "    return R\n",
    "\n",
    "def Augmentation_clock(x,y):\n",
    "\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    # print(y)\n",
    "    y_aug = np.transpose(np.matmul(rotation_matrix(-90), np.transpose(y)))\n",
    "    # print(y_aug)\n",
    "\n",
    "    temp = x.copy()\n",
    "    x0 = temp[:,0]\n",
    "    x1 = temp[:,1]\n",
    "    x2 = temp[:,2]\n",
    "    x3 = temp[:,3]\n",
    "\n",
    "    x[:,0] = x3\n",
    "    x[:,1] = x0\n",
    "    x[:,2] = x1\n",
    "    x[:,3] = x2\n",
    " \n",
    "    return x,y_aug\n",
    "\n",
    "def Augmentation_flip(x,y):\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    x = np.flip(x, axis=1)\n",
    "    y[:,1] = -1 * y[:,1]\n",
    "    return x,y\n",
    "\n",
    "def Augmentation_anticlock(x,y):\n",
    "\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    y_aug = np.transpose(np.matmul(rotation_matrix(90), np.transpose(y)))\n",
    "\n",
    "    temp = x.copy()\n",
    "    x0 = temp[:,0]\n",
    "    x1 = temp[:,1]\n",
    "    x2 = temp[:,2]\n",
    "    x3 = temp[:,3]\n",
    "\n",
    "    x[:,0] = x1\n",
    "    x[:,1] = x2\n",
    "    x[:,2] = x3\n",
    "    x[:,3] = x0\n",
    " \n",
    "    return x,y_aug\n",
    "\n",
    "def data_augmentation(x,y):\n",
    "    x = x.numpy()\n",
    "    y = y.numpy()\n",
    "    x_aug1,y_aug1 = Augmentation_clock(x, y)\n",
    "    x_aug2,y_aug2 = Augmentation_clock(x_aug1,y_aug1)\n",
    "    x_aug3,y_aug3 = Augmentation_clock(x_aug2,y_aug2)\n",
    "    x_aug4,y_aug4 = Augmentation_flip(x_aug3,y_aug3)\n",
    "    x_aug5,y_aug5 = Augmentation_clock(x_aug4,y_aug4)\n",
    "    x_aug6,y_aug6 = Augmentation_clock(x_aug5,y_aug5)\n",
    "    x_aug7,y_aug7 = Augmentation_clock(x_aug6,y_aug6)\n",
    "    X_train_Aug = np.concatenate((x, x_aug1, x_aug2, x_aug3, x_aug4, x_aug5, x_aug6, x_aug7))\n",
    "    Y_train_Aug = np.concatenate((y, y_aug1, y_aug2, y_aug3, y_aug4, y_aug5, y_aug6, y_aug7))\n",
    "\n",
    "    X_train_Aug = tf.convert_to_tensor(X_train_Aug)\n",
    "    Y_train_Aug = tf.convert_to_tensor(Y_train_Aug)\n",
    "\n",
    "    return X_train_Aug, Y_train_Aug\n",
    "\n",
    "X_train_Aug, Y_train_Aug = data_augmentation(X_train, Y_train)\n",
    "# X_validation_Aug, Y_validation_Aug = data_augmentation(X_validation, Y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network with subclassing api on Normal data\n",
    "\n",
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self, units, kernel_regularizer):\n",
    "        super(Hidden_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],self.units), initializer=initializer,\n",
    "                                 trainable=True, regularizer=self.kernel_regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units, kernel_regularizer):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],self.units), initializer=initializer,\n",
    "                                 trainable=True, regularizer=self.kernel_regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "# class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.dense1 = Hidden_layer(4, kernel_regularizer=kernel_regularizer)\n",
    "#         # self.bn = tf.keras.layers.BatchNormalization()\n",
    "#         self.dense2_1 = Output_layer(2, kernel_regularizer=kernel_regularizer)\n",
    "#         self.relu = MyReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.relu(self.dense1(x))\n",
    "#         return self.dense2_1(x)\n",
    "\n",
    "#     def build_graph(self):\n",
    "#         x = tf.keras.layers.Input(shape=(1,4))\n",
    "#         return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(Hidden_layer(4, kernel_regularizer=kernel_regularizer))\n",
    "# model.add(MyReLU())\n",
    "# model.add(Output_layer(2, kernel_regularizer=kernel_regularizer))\n",
    "# # model.build_graph().summary()\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "# )\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "#                     callbacks=callbacks,\n",
    "#                     # shuffle=True\n",
    "#                     )\n",
    "\n",
    "# predictions = model.predict(X_validation)\n",
    "# prediction_accuracy(predictions, Y_validation)\n",
    "# learning_curves(history)\n",
    "# evaluate_train = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "\n",
    "# evaluate_train = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "# evaluate_validation = model.evaluate(X_validation,Y_validation, batch_size=batch_size)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000\n",
    "# steps_per_epoch = sum(train_occurences) / batch_size\n",
    "starter_learning_rate = 1e-1\n",
    "decay_steps = epochs * 3\n",
    "scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "    decay_steps= decay_steps,\n",
    "    end_learning_rate= end_learning_rate,\n",
    "    power=1)\n",
    "kernel_regularizer=tf.keras.regularizers.L1L2(0.01)\n",
    "callbacks=[keras.callbacks.EarlyStopping(monitor='val_mean_squared_error',patience=20)]\n",
    "initializer=tf.keras.initializers.HeUniform\n",
    "\n",
    "results = pd.DataFrame(columns=['start_learning_rate', 'width', 'depth', 'l2_weight', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])\n",
    "\n",
    "starter_learning_rate = [1e-1]\n",
    "batch_sizes = [32, 64]\n",
    "widths = [4]\n",
    "depths = [1, 2, 4]\n",
    "# regularizer_strength = [0.01]\n",
    "# kernel_regularizer=[tf.keras.regularizers.L1L2(0.01)]\n",
    "l2_weights = [0.01, 0.001, 1e-4]\n",
    "# make cross product\n",
    "\n",
    "for starter_learning_rate in starter_learning_rate:\n",
    "    for width in widths:\n",
    "        for depth in depths:\n",
    "            for l2_weight in l2_weights:\n",
    "                for batch_size in batch_sizes:\n",
    "                    model = keras.models.Sequential()\n",
    "                    model.add(Hidden_layer(4, kernel_regularizer=keras.regularizers.l2(l2_weight)))\n",
    "                    model.add(MyReLU())\n",
    "                    for _ in range(depth):\n",
    "                        model.add(Hidden_layer(4, kernel_regularizer=keras.regularizers.l2(l2_weight)))\n",
    "                        model.add(MyReLU())\n",
    "                    model.add(Output_layer(2, kernel_regularizer=keras.regularizers.l2(l2_weight)))\n",
    "                    scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "                                decay_steps= decay_steps,\n",
    "                                end_learning_rate= end_learning_rate,\n",
    "                                power=1)\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "                                    loss='mean_squared_error',\n",
    "                                    metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "                                    )\n",
    "                    history = model.fit(X_train, Y_train, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                                        callbacks=callbacks,\n",
    "                                        # shuffle=True\n",
    "                                        )\n",
    "                    train_loss, train_acc = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "                    val_loss, val_acc = model.evaluate(X_validation,Y_validation, batch_size=batch_size)\n",
    "                    results_tmp = np.array([starter_learning_rate, width, depth, l2_weight, train_loss, val_loss, train_acc, val_acc]).reshape(1, -1)\n",
    "                    results = results.append(pd.DataFrame(data=results_tmp, columns=results.columns), ignore_index=True)\n",
    "results.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7e9f4964f105db5d476a6bf9b47e70d0bf16854eb8b7ea4fd7f47088c3e55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
