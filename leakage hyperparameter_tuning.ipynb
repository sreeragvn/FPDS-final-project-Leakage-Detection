{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to do/doubts/discussions\n",
    "# model sometimes give constant loss over epochs and gives poor results on prediction\n",
    "# is the loss value supposed to reduce continously\n",
    "# should we do augmentation of validation data ?\n",
    "# All values are already between -1 to +1. should we do scaling on top of this ?\n",
    "# if we are adding scaling of data, how to ensure that when it is being tested would give out rescaled data\n",
    "\n",
    "\n",
    "# how to save subclassing api\n",
    "# hyperparameter tuning\n",
    "# how to save the best model among all the epochs\n",
    "# should we shuffle repeat, prefetch etc ?\n",
    "# incase of early stopping - what metric is to be monitored - val loss or val mse ?\n",
    "\n",
    "#observations\n",
    "#load model works without regularizer and initializer being passed  as arguments during forward pass\n",
    "\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "data_size = 1000\n",
    "X_train, Y_train, X_validation, Y_validation, X_test = load_data(data_size)\n",
    "\n",
    "# generating augmented data\n",
    "X_train_Aug, Y_train_Aug = data_augmentation(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self,units, **kwargs):\n",
    "        super(Hidden_layer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name = 'w',shape=(input_shape[-1],self.units), initializer=tf.keras.initializers.HeUniform(seed =22),\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.keras.activations.relu(tf.matmul(inputs, self.W))\n",
    "        return x\n",
    "    def get_config(self):\n",
    "        config = super(Hidden_layer, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        # config.update({\"initializer\": initializer})\n",
    "        # config.update({\"kernel_regularizer\": kernel_regularizer})\n",
    "        return config\n",
    "        # return {\"units\": self.units, \"kernel_regularizer\": kernel_regularizer, \"initializer\": initializer}\n",
    "    # @classmethod\n",
    "    # def from_config(cls, config):\n",
    "    #     return cls(**config)\n",
    "        \n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(Output_layer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name = 'w',shape=(input_shape[-1],self.units), initializer=tf.keras.initializers.HeUniform(seed = 22),\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return tf.keras.activations.tanh(x)\n",
    "    def get_config(self):\n",
    "        config = super(Output_layer, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        # config.update({\"initializer\": initializer})\n",
    "        # config.update({\"kernel_regularizer\": kernel_regularizer})\n",
    "        return config\n",
    "        # return {\"units\": self.units, \"kernel_regularizer\": kernel_regularizer, \"initializer\": initializer}\n",
    "    # @classmethod\n",
    "    # def from_config(cls, config):\n",
    "    #     return cls(**config)\n",
    "\n",
    "# class MyReLU(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(MyReLU, self).__init__()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return tf.math.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['learning_rate', 'width', 'depth', 'batch_size', 'train_loss', 'val_loss', 'train_mse', 'val_mse'])\n",
    "\n",
    "# @neeraj Make sure that you are printing all the values you are looping and update results dataframe\n",
    "\n",
    "#build the for loop for all parameters or use cross product\n",
    "\n",
    "# defining the parameters\n",
    "epochs = 10000\n",
    "verbose=2\n",
    "\n",
    "learning_rate = [1e-1,1e-2,1e-3,1e-4]\n",
    "batch_sizes = [32, 64]\n",
    "widths = [2, 3, 4, 32, 64, 128]\n",
    "depths = [0, 1, 2, 4, 5]\n",
    "# add a loop for the types of loss function to consider\n",
    "    # Mean Squared Error\n",
    "    # Root Mean Squared Error\n",
    "    # Mean Absolute Error\n",
    "    # mean absolute error\n",
    "    # other losses can be found at https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# add a loop for the type of optimizer to consider - sgd, rmsprop and adam \n",
    "\n",
    "# make cross product\n",
    "# This loop has a problem- it set number of neurons for all layers in an iteration- see if you can fix it if possible\n",
    "\n",
    "for learning_rate in learning_rate:\n",
    "    for width in widths:\n",
    "        for depth in depths:\n",
    "            for batch_size in batch_sizes:\n",
    "                    model = keras.models.Sequential()\n",
    "                    model.add(Hidden_layer(widths))\n",
    "                    for _ in range(depth):\n",
    "                        model.add(Hidden_layer(widths))\n",
    "                    model.add(Output_layer(2))\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                    loss = [tf.keras.losses.MeanAbsoluteError()],\n",
    "                                    metrics = tf.keras.metrics.MeanSquaredError()\n",
    "                                )\n",
    "                    history = model.fit(X_train, Y_train, \n",
    "                                        epochs=epochs, \n",
    "                                        batch_size= batch_size, \n",
    "                                        verbose=verbose,\n",
    "                                        validation_data=(X_validation, Y_validation),\n",
    "                                        # callbacks=callbacks,\n",
    "                                        # shuffle=True\n",
    "                                        )\n",
    "                    train_loss, train_mse = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "                    val_loss, val_mse = model.evaluate(X_validation,Y_validation, batch_size=batch_size)\n",
    "                    results_tmp = np.array([learning_rate, width, depth, batch_size, train_loss, val_loss, train_mse, val_mse]).reshape(1, -1)\n",
    "                    # either use tensor board or save the training curve in each loop. fuction for plotting training curve is available in utility\n",
    "                    #make sure every variable in loop is in the right order \n",
    "                    results = results.append(pd.DataFrame(data=results_tmp, columns=results.columns), ignore_index=True)\n",
    "results.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7e9f4964f105db5d476a6bf9b47e70d0bf16854eb8b7ea4fd7f47088c3e55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
