{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:44:29.506863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:44:30.687562: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-02-05 19:44:30.687587: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: vn\n",
      "2023-02-05 19:44:30.687592: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: vn\n",
      "2023-02-05 19:44:30.687693: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.108.3\n",
      "2023-02-05 19:44:30.687708: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.108.3\n",
      "2023-02-05 19:44:30.687713: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.108.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "# from sklearn import preprocessing\n",
    "# Make numpy values easier to read.\n",
    "# np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices(\"gpu\")\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "# np.random.seed(101)\n",
    "# tf.random.set_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1845568117.py, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 64\u001b[0;36m\u001b[0m\n\u001b[0;31m    Y_train = Y_train.to_numpy()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# preprocessing of data\n",
    "\n",
    "leakage_train_100 = pd.read_csv(\"leakage_dataset_train_100.csv\")\n",
    "leakage_train_1000 = pd.read_csv(\"leakage_dataset_train_1000.csv\")\n",
    "leakage_val_1000 = pd.read_csv(\"leakage_dataset_validation_1000.csv\")\n",
    "\n",
    "train_ds = leakage_train_100\n",
    "val_ds = leakage_val_1000\n",
    "\n",
    "train_ds = train_ds.sample(frac=1)\n",
    "val_ds = val_ds.sample(frac=1)\n",
    " \n",
    "batch_size = 25\n",
    "\n",
    "# train_ds = train_ds.repeat()\n",
    "# train_ds = train_ds.shuffle(buffer_size=1024, seed=0)\n",
    "# train_ds = train_ds.batch(batch_size=batch_size)\n",
    "# train_ds = train_ds.prefetch(buffer_size=1)\n",
    "\n",
    "# val_ds = val_ds.batch(batch_size=batch_size)\n",
    "# val_ds = val_ds.prefetch(buffer_size=1)\n",
    "\n",
    "\n",
    "# leakage_train_100.head()\n",
    "# leakage_val_1000.head()\n",
    "\n",
    "# #create two output arrays\n",
    "# def format_output(data):\n",
    "#     data = data.copy()\n",
    "#     y1 = data.pop('y1')\n",
    "#     y1 = np.array(y1)\n",
    "#     y2 = data.pop('y2')\n",
    "#     y2 = np.array(y2)\n",
    "#     return y1, y2\n",
    "\n",
    "X_train = train_ds.iloc[:,2:].to_numpy()\n",
    "Y_train = train_ds.iloc[:,:2]\n",
    "\n",
    "#splitting of x and y variable\n",
    "# X_train = leakage_train_1000.iloc[:,2:].to_numpy()\n",
    "# Y_train = leakage_train_1000.iloc[:,:2]\n",
    "\n",
    "num_rows, num_cols = X_train.shape\n",
    "\n",
    "X_validation = val_ds.iloc[:,2:].to_numpy()\n",
    "Y_validation = val_ds.iloc[:,:2]\n",
    "\n",
    "scX = preprocessing.StandardScaler()\n",
    "stY = preprocessing.StandardScaler()\n",
    "\n",
    "def data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY):\n",
    "    X_train = scX.fit_transform(X_train)\n",
    "    X_validation = scX.transform(X_validation)\n",
    "\n",
    "    Y_train = stY.fit_transform(Y_train)\n",
    "    Y_validation = stY.transform(Y_validation)\n",
    "\n",
    "    return X_train, X_validation, Y_train, Y_validation\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY)\n",
    "# Y_train = pd.DataFrame(Y_train, columns = ['y1','y2'])\n",
    "# Y_validation = pd.DataFrame(Y_validation, columns = ['y1','y2'])\n",
    "\n",
    "# y1_train, y2_train = format_output(Y_train)\n",
    "# y1_validation, y2_validation = format_output(Y_validation)\n",
    "\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_validation = Y_validation.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN architechture - keras Functional API\n",
    "# Now we have one input layer, 1 hidden layer and 2 output layer - both connected to hidden layer\n",
    "# Thinking of adding a normalization layer or doing normalization before training\n",
    "# inputs = tf.keras.layers.Normalization(input_shape=[4,], axis=None)\n",
    "inputs = tf.keras.Input(shape=(4,))\n",
    "dense1 = tf.keras.layers.Dense(4, activation='relu')\n",
    "# dense2_1 = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'y1')\n",
    "# dense2_2 = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'y2')\n",
    "\n",
    "# dense2_1 = tf.keras.layers.Dense(1, name = 'y1')\n",
    "# dense2_2 = tf.keras.layers.Dense(1, name = 'y2')\n",
    "dense2_1 = tf.keras.layers.Dense(2)\n",
    "\n",
    "x=dense1(inputs)\n",
    "\n",
    "outputs1=dense2_1(x)\n",
    "# outputs2=dense2_2(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs1, name = 'leakge_functional_model')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "starter_learning_rate = 1e-4\n",
    "end_learning_rate = 1e-8\n",
    "decay_steps = 10000\n",
    "scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "    decay_steps= decay_steps,\n",
    "    end_learning_rate= end_learning_rate,\n",
    "    power=1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=5, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "\n",
    "# even for a primilinary result, the loss is pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves(history)   :\n",
    "    sns.set_style('darkgrid', {'axes.facecolor': '.9'})\n",
    "    sns.set_context('notebook')\n",
    "\n",
    "    # your code\n",
    "    ### Learning curves\n",
    "    history_frame = pd.DataFrame(history.history)\n",
    "    history_frame.plot(figsize=(8, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(predictions, Y_validation): \n",
    "    predictions = predictions.transpose()\n",
    "    Y_validation = Y_validation.transpose()\n",
    "    y1 = predictions[0]\n",
    "    y2 = predictions[1]\n",
    "    y1_validation = Y_validation[0]\n",
    "    y2_validation = Y_validation[1]\n",
    "    fig, axs = plt.subplots(2)\n",
    "    # print(y1_validation.shape, y1.shape)\n",
    "    # print(y2_validation.shape, y2.shape)\n",
    "    # fig.suptitle('')\n",
    "    axs[0].scatter(y1_validation, y1)\n",
    "    axs[0].set_title('y1')\n",
    "    axs[1].scatter(y2_validation, y2)\n",
    "    axs[1].set_title('y2')\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='true value', ylabel='predicted value')\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    print(\"rmse of y1: \", mean_squared_error(y1_validation, y1, squared=False))\n",
    "    print(\"rmse of y2: \", mean_squared_error(y2_validation, y2, squared=False))\n",
    "\n",
    "prediction_accuracy(predictions, Y_validation)\n",
    "learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Augmentation\n",
    "# Requires cleaning up\n",
    "\n",
    "def rotation_matrix(angle):\n",
    "    theta = np.radians(angle)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c, -s), (s, c)))\n",
    "    return R\n",
    "\n",
    "def Augmentation_clock(x,y):\n",
    "\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    # print(y)\n",
    "    y_aug = np.transpose(np.matmul(rotation_matrix(-90), np.transpose(y)))\n",
    "    # print(y_aug)\n",
    "\n",
    "    temp = x.copy()\n",
    "    x0 = temp[:,0]\n",
    "    x1 = temp[:,1]\n",
    "    x2 = temp[:,2]\n",
    "    x3 = temp[:,3]\n",
    "\n",
    "    # print(x0.shape)\n",
    "\n",
    "    x[:,0] = x3\n",
    "    x[:,1] = x0\n",
    "    x[:,2] = x1\n",
    "    x[:,3] = x2\n",
    " \n",
    "    return x,y_aug\n",
    "\n",
    "\n",
    "def Augmentation_flip(x,y):\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    x = np.flip(x, axis=1)\n",
    "    y[:,1] = -1 * y[:,1]\n",
    "    return x,y\n",
    "\n",
    "def Augmentation_anticlock(x,y):\n",
    "\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    y_aug = np.transpose(np.matmul(rotation_matrix(90), np.transpose(y)))\n",
    "\n",
    "    temp = x.copy()\n",
    "    x0 = temp[:,0]\n",
    "    x1 = temp[:,1]\n",
    "    x2 = temp[:,2]\n",
    "    x3 = temp[:,3]\n",
    "\n",
    "    x[:,0] = x1\n",
    "    x[:,1] = x2\n",
    "    x[:,2] = x3\n",
    "    x[:,3] = x0\n",
    " \n",
    "    return x,y_aug\n",
    "\n",
    "# # test inputs\n",
    "# X_train = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "# # print(X_train)\n",
    "# Y_train = np.array([[15,16], [17,18], [19,20]])\n",
    "# num_rows, num_cols = X_train.shape\n",
    "\n",
    "def data_augmentation(x,y):\n",
    "    x_aug1,y_aug1 = Augmentation_clock(x, y)\n",
    "    x_aug2,y_aug2 = Augmentation_clock(x_aug1,y_aug1)\n",
    "    x_aug3,y_aug3 = Augmentation_clock(x_aug2,y_aug2)\n",
    "    x_aug4,y_aug4 = Augmentation_flip(x_aug3,y_aug3)\n",
    "    x_aug5,y_aug5 = Augmentation_clock(x_aug4,y_aug4)\n",
    "    x_aug6,y_aug6 = Augmentation_clock(x_aug5,y_aug5)\n",
    "    x_aug7,y_aug7 = Augmentation_clock(x_aug6,y_aug6)\n",
    "    X_train_Aug = np.concatenate((x, x_aug1, x_aug2, x_aug3, x_aug4, x_aug5, x_aug6, x_aug7))\n",
    "    Y_train_Aug = np.concatenate((y, y_aug1, y_aug2, y_aug3, y_aug4, y_aug5, y_aug6, y_aug7))\n",
    "\n",
    "    return X_train_Aug, Y_train_Aug\n",
    "\n",
    "X_train_Aug, Y_train_Aug = data_augmentation(X_train, Y_train)\n",
    "X_validation_Aug, Y_validation_Aug = data_augmentation(X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42) \n",
    "\n",
    "# Model trained on Augmented dataset\n",
    "# Now we have one input layer, 1 hidden layer and 2 output layer - both connected to hidden layer\n",
    "# Thinking of adding a normalization layer or doing normalization before training\n",
    "# inputs = tf.keras.layers.Normalization(input_shape=[4,], axis=None)\n",
    "inputs = tf.keras.Input(shape=(4,))\n",
    "dense1 = tf.keras.layers.Dense(4, activation='relu')\n",
    "# dense2_1 = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'y1')\n",
    "# dense2_2 = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'y2')\n",
    "\n",
    "# dense2_1 = tf.keras.layers.Dense(1, name = 'y1')\n",
    "# dense2_2 = tf.keras.layers.Dense(1, name = 'y2')\n",
    "dense2_1 = tf.keras.layers.Dense(2)\n",
    "\n",
    "x=dense1(inputs)\n",
    "\n",
    "outputs1=dense2_1(x)\n",
    "# outputs2=dense2_2(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs1, name = 'leakge_aug_functional_model')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "starter_learning_rate = 1e-4\n",
    "end_learning_rate = 1e-8\n",
    "decay_steps = 10000\n",
    "scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "    decay_steps= decay_steps,\n",
    "    end_learning_rate= end_learning_rate,\n",
    "    power=1)\n",
    "\n",
    "history = model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "# y_aug = {\n",
    "#     \"y1\" : Y_train_Aug[:,0],\n",
    "#     \"y2\" : Y_train_Aug[:,1]\n",
    "# }\n",
    "\n",
    "# y_aug_valid = {\n",
    "#     \"y1\" : Y_validation_Aug[:,0],\n",
    "#     \"y2\" : Y_validation_Aug[:,1]\n",
    "# }\n",
    "\n",
    "history = model.fit(X_train_Aug, y=Y_train_Aug, epochs=5, batch_size= batch_size, verbose=2, validation_data=(X_validation_Aug, Y_validation_Aug),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "predictions = model.predict(X_validation_Aug)\n",
    "\n",
    "# even for a primilinary result, the loss is pretty good\n",
    "\n",
    "prediction_accuracy(predictions, Y_validation_Aug)\n",
    "learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivariant NN - by averaging weights\n",
    "# how to make constraint work on subclassing api\n",
    "# Equivariance NN with subclassing API - experiment with layer constraint\n",
    "# These function make use of the subclassing Api and custom layer functionality of tensorflow\n",
    "# experiment\n",
    "\n",
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self, units, kernel_constraint):\n",
    "        super(Hidden_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.kernel_constraint = kernel_constraint\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=tf.keras.initializers.GlorotNormal, \n",
    "            trainable=True, constraint=self.kernel_constraint)\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class Hidden_layer_constraint(tf.keras.constraints.Constraint):\n",
    "    # def __init__(self):\n",
    "    #     self.d = None\n",
    "\n",
    "    # @tf.function\n",
    "    def __call__(self, w):\n",
    "        a = tf.reduce_mean([w[0,0], w[1,1], w[2,2], w[3,3]])\n",
    "        b = tf.reduce_mean([w[0,1], w[1,0], w[1,2], w[2,1], w[0,3], w[3,0], w[2,3], w[3,2]])\n",
    "        c = tf.reduce_mean([w[0,2], w[1,3], w[2,0], w[3,1]])\n",
    "        # a = tf.reduce_mean(w)\n",
    "        # b = tf.reduce_mean(w)+1\n",
    "        # c = tf.reduce_mean(w)+2\n",
    "        \n",
    "        row_indices = tf.constant([0, 1, 2, 3])\n",
    "        col_indices = tf.constant([0, 1, 2, 3])\n",
    "        w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(a, tf.shape(row_indices)[0]))\n",
    "\n",
    "        row_indices = tf.constant([0, 0, 1, 1, 2, 2, 3, 3])\n",
    "        col_indices = tf.constant([1, 3, 2, 0, 1, 3, 0, 2])\n",
    "        w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(b, tf.shape(row_indices)[0]))\n",
    "\n",
    "        row_indices = tf.constant([0, 1, 3, 2])\n",
    "        col_indices = tf.constant([2, 3, 1, 0])\n",
    "        w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(c, tf.shape(row_indices)[0]))\n",
    "\n",
    "        return w\n",
    "\n",
    "class Output_layer_constraint(tf.keras.constraints.Constraint):\n",
    "    # def __init__(self):\n",
    "    #     self.d = None\n",
    "\n",
    "    # @tf.function\n",
    "    def __call__(self, w):\n",
    "        self.d = tf.reduce_mean(w)\n",
    "        w = w/w * self.d\n",
    "        row_indices = tf.constant([0, 1, 1, 2])\n",
    "        col_indices = tf.constant([1, 0, 1, 0])\n",
    "        w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(-self.d, tf.shape(row_indices)[0]))\n",
    "        return w\n",
    "\n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units, kernel_constraint):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.kernel_constraint = kernel_constraint\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),initializer=tf.keras.initializers.GlorotNormal, trainable=True, constraint=self.kernel_constraint\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "    def __init__(self):\n",
    "        super(MyModel, self). __init__()\n",
    "        self.dense1 = Hidden_layer(4, kernel_constraint=Hidden_layer_constraint())\n",
    "        self.dense2 = Hidden_layer(4, kernel_constraint=Hidden_layer_constraint())\n",
    "        # self.dense2_1 = tf.keras.layers.Dense(2, kernel_constraint=Output_layer_constraint())\n",
    "        self.dense3_1 = Output_layer(2, kernel_constraint=Output_layer_constraint())\n",
    "        self.relu = MyReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.relu(self.dense2(x))\n",
    "        # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "        return self.dense3_1(x)\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = tf.keras.layers.Input(shape=(1,4))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "model = MyModel()\n",
    "model.build_graph().summary()\n",
    "\n",
    "starter_learning_rate = 1e-1\n",
    "end_learning_rate = 1e-4\n",
    "decay_steps = 10000\n",
    "scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "    decay_steps= decay_steps,\n",
    "    end_learning_rate= end_learning_rate,\n",
    "    power=1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                    # callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    "                    )\n",
    "\n",
    "print(\"output layer weight matrix\")\n",
    "print(model.layers[2].weights)\n",
    "\n",
    "\n",
    "print(\"Hidden layer weight matrix\")\n",
    "print(model.layers[1].weights)\n",
    "\n",
    "print(\"Hidden layer weight matrix\")\n",
    "print(model.layers[0].weights)\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "prediction_accuracy(predictions, Y_validation)\n",
    "learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Functional Tuning - Option 2: using Keras Tuner ------------------------------\n",
    "# # Goal: tune the learning rate\n",
    "\n",
    "# # 0. Install and import all the packages needed\n",
    "# #!pip install -q -U keras-tuner\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# # 1. Define the general architecture of the model through a creation user-defined function\n",
    "# def model_builder(hp):\n",
    "#   model = Sequential()\n",
    "#   model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n",
    "#   model.add(Dense(8, activation='relu'))\n",
    "#   model.add(Dense(1))\n",
    "#   hp_learning_rate = hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3, 1e-4]) # Tuning the learning rate (four different values to test: 0.1, 0.01, 0.001, 0.0001)\n",
    "#   #optimizer = RMSprop(learning_rate = hp_learning_rate) \n",
    "#   optimizer = tf.keras.optimizers.Adam(learning_rate = hp_learning_rate) # Defining the optimizer\n",
    "#   model.compile(loss='mse',metrics=['mse'], optimizer=optimizer)                   # Compiling the model \n",
    "#   return model                                                                     # Returning the defined model\n",
    "\n",
    "# # 2. Define the hyperparameters grid to be validated\n",
    "# tuner_rs = kt.RandomSearch(\n",
    "#               model_builder,                # Takes hyperparameters (hp) and returns a Model instance\n",
    "#               objective = 'mse',            # Name of model metric to minimize or maximize\n",
    "#               seed = 42,                    # Random seed for replication purposes\n",
    "#               max_trials = 5,               # Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested.\n",
    "#               directory='random_search')    # Path to the working directory (relative).\n",
    "\n",
    "# # 3. Run the GridSearchCV process\n",
    "# tuner_rs.search(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea to explore - \n",
    "# 1. return only a,b,c from the layer then in the forward pass declare the weight matrix and do the layer operation\n",
    "# 2. return only a,b,c from the layer and then in the constraint declare the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not important Experiment 2 - learned how to make a neural network with subclassing api\n",
    "# # These function make use of the subclassing Api and custom layer functionality of tensorflow\n",
    "# # experiment\n",
    "\n",
    "# class Hidden_layer(layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Hidden_layer, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.a = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         self.b = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         self.c = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         x= tf.Variable(\n",
    "#             [self.a, self.b, self.c, self.b, \n",
    "#              self.b, self.a, self.b, self.c, \n",
    "#              self.c, self.b, self.a, self.b, \n",
    "#              self.b, self.c, self.b, self.a])\n",
    "#         x = tf.reshape(x, shape=(input_shape[-1],self.units))\n",
    "#         self.W = tf.Variable(x, shape=(input_shape[-1],self.units),trainable=True)\n",
    "#     def call(self, inputs):\n",
    "#         # print(self.W.shape)\n",
    "#         # print(inputs.shape)\n",
    "#         # # print(\"a\")\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         # print(\"b\")\n",
    "#         # print(x)\n",
    "#         # print(x.shape)\n",
    "#         return x\n",
    "\n",
    "# class Output_layer(layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Output_layer, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.d = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         x = tf.Variable([self.d,-self.d,-self.d,self.d, -self.d,-self.d,self.d,self.d])\n",
    "#         x = r = tf.reshape(x, shape=(input_shape[-1],self.units))\n",
    "#         self.W = tf.Variable(x, shape=(input_shape[-1],self.units),trainable=True)\n",
    "        \n",
    "#         # self.W = tf.Variable(\n",
    "#         #     [[self.d,-self.d],\n",
    "#         #     [-self.d,-self.d], \n",
    "#         #      [-self.d,-self.d],\n",
    "#         #      [self.d,self.d]],shape=(input_shape[-1],self.units),\n",
    "#         #     trainable=True)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # inputs = inputs\n",
    "#         # W = self.W.transpose()\n",
    "#         # print(self.W.shape)\n",
    "#         # print(inputs.shape)\n",
    "#         # print(\"a\")\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         # print(\"b\")\n",
    "#         # print(x.shape)\n",
    "#         return x\n",
    "\n",
    "# class MyReLU(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(MyReLU, self).__init__()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return tf.math.maximum(x, 0)\n",
    "\n",
    "# class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.dense1 = Hidden_layer(4)\n",
    "#         self.dense2_1 = Output_layer(2)\n",
    "#         # self.dense2_2 = Output_layer(1)\n",
    "#         self.relu = MyReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.relu(self.dense1(x))\n",
    "#         # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "#         return self.dense2_1(x)\n",
    "\n",
    "#     def build_graph(self):\n",
    "#         x = tf.keras.layers.Input(shape=(1,4))\n",
    "#         return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "# model = MyModel()\n",
    "# model.build_graph().summary()\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y=Y_train, epochs=5, batch_size=5, verbose=2)\n",
    "\n",
    "# print(\"output layer weight\")\n",
    "# print(model.layers[1].weights)\n",
    "\n",
    "# print(\"hidden layer weight\")\n",
    "# print(model.layers[0].weights)\n",
    "\n",
    "# predictions = model.predict(X_validation)\n",
    "# prediction_accuracy(predictions, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not important Experiment 3 - learned how to give custom kernel constraint\n",
    "# # how to make constraint work on subclassing api\n",
    "# # Equivariance NN with subclassing API - experiment with layer constraint\n",
    "# # These function make use of the subclassing Api and custom layer functionality of tensorflow\n",
    "# # experiment\n",
    "\n",
    "# class Hidden_layer(layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Hidden_layer, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[-1], self.units),initializer=tf.keras.initializers.GlorotNormal, trainable=True)\n",
    "            \n",
    "#     def call(self, inputs):\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         return x\n",
    "\n",
    "# class Output_layer_constraint(tf.keras.constraints.Constraint):\n",
    "#     # def __call__(self, d):\n",
    "#     #     self.d = d\n",
    "#     def __call__(self, w):\n",
    "#         # x = tf.Variable([self.d,-self.d,-self.d,self.d, -self.d,-self.d,self.d,self.d])\n",
    "#         # w = tf.reshape(x, shape=(4.4))\n",
    "    \n",
    "#         return w/w\n",
    "\n",
    "# class Output_layer(layers.Layer):\n",
    "#     def __init__(self, units, kernel_constraint):\n",
    "#         super(Output_layer, self).__init__()\n",
    "#         self.units = units\n",
    "#         self.kernel_constraint = kernel_constraint\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[-1], self.units),initializer=tf.keras.initializers.GlorotNormal, trainable=True, constraint=self.kernel_constraint\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         return x\n",
    "\n",
    "# class MyReLU(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(MyReLU, self).__init__()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return tf.math.maximum(x, 0)\n",
    "\n",
    "# class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.dense1 = Hidden_layer(4)\n",
    "#         # self.dense2_1 = tf.keras.layers.Dense(2, kernel_constraint=Output_layer_constraint())\n",
    "#         self.dense2_1 = Output_layer(2, kernel_constraint=Output_layer_constraint())\n",
    "#         self.relu = MyReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.relu(self.dense1(x))\n",
    "#         # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "#         return self.dense2_1(x)\n",
    "\n",
    "#     # def build_graph(self):\n",
    "#     #     x = tf.keras.layers.Input(shape=(1,4))\n",
    "#     #     return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "# model = MyModel()\n",
    "# # model.build_graph().summary()\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y=Y_train, epochs=5, batch_size=5, verbose=2)\n",
    "# print(model.layers[1].weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7e9f4964f105db5d476a6bf9b47e70d0bf16854eb8b7ea4fd7f47088c3e55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
