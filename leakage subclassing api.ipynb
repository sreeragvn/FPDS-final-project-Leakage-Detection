{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to do/doubts/discussions\n",
    "# how to save subclassing api\n",
    "# do regularization\n",
    "# hyperparameter tuning\n",
    "# how to save the best model among all the epochs\n",
    "# should we do augmentation of validation data ?\n",
    "# should we shuffle repeat, prefetch etc ?\n",
    "# All values are already between -1 to +1. should we do scaling ?\n",
    "# if we are adding scaling of data, how to ensure that when it is being tested by brauer our code would give out rescaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "# from sklearn import preprocessing\n",
    "# Make numpy values easier to read.\n",
    "# np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices(\"gpu\")\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# np.random.seed(101)\n",
    "# tf.random.set_seed(101)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data of data\n",
    "\n",
    "leakage_train_100 = pd.read_csv(\"leakage_dataset_train_100.csv\")\n",
    "leakage_train_1000 = pd.read_csv(\"leakage_dataset_train_1000.csv\")\n",
    "leakage_val_1000 = pd.read_csv(\"leakage_dataset_validation_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration\n",
    "# print(leakage_train_100.columns)\n",
    "# print(leakage_train_1000.columns)\n",
    "# print(leakage_val_1000.columns)\n",
    "\n",
    "# def data_description(datset):\n",
    "#     print(datset['y1'].describe())\n",
    "#     print(datset['y2'].describe())\n",
    "#     print(datset['mfc1'].describe())\n",
    "#     print(datset['mfc2'].describe())\n",
    "#     print(datset['mfc3'].describe())\n",
    "#     print(datset['mfc4'].describe())\n",
    "\n",
    "# data_description(leakage_train_100)\n",
    "# data_description(leakage_train_1000)\n",
    "# data_description(leakage_val_1000)\n",
    "\n",
    "# sns.set()\n",
    "# cols = ['y1', 'y2', 'mfc1', 'mfc2', 'mfc3', 'mfc4']\n",
    "# sns.pairplot(leakage_train_1000[cols], size = 2.5)\n",
    "# plt.show()\n",
    "\n",
    "# missing data\n",
    "# def missingdata(df_train):\n",
    "#     total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "#     percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "#     missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "#     print(missing_data.head(20))\n",
    "\n",
    "# missingdata(leakage_val_1000)\n",
    "\n",
    "# histogram and normal probability plot\n",
    "# from scipy.stats import norm\n",
    "# from scipy import stats\n",
    "\n",
    "# sns.distplot(leakage_val_1000['y1'], fit=norm)\n",
    "# fig = plt.figure()\n",
    "# res = stats.probplot(leakage_val_1000['y1'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m X_train, Y_train, X_validation, Y_validation \u001b[39m=\u001b[39m load_data(leakage_train_100)\n\u001b[1;32m     22\u001b[0m \u001b[39m#test dataset\u001b[39;00m\n\u001b[1;32m     23\u001b[0m X_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m0.25\u001b[39m, \u001b[39m0.25\u001b[39m, \u001b[39m0.25\u001b[39m,\u001b[39m0.25\u001b[39m], \n\u001b[1;32m     24\u001b[0m                     [\u001b[39m0.35\u001b[39m, \u001b[39m0.15\u001b[39m, \u001b[39m0.25\u001b[39m,\u001b[39m0.25\u001b[39m], \n\u001b[1;32m     25\u001b[0m                     [\u001b[39m0.25\u001b[39m, \u001b[39m0.25\u001b[39m, \u001b[39m0.15\u001b[39m,\u001b[39m0.35\u001b[39m],\n\u001b[0;32m---> 26\u001b[0m                     [\u001b[39m0.15\u001b[39;49m, \u001b[39m0.35\u001b[39;49m, \u001b[39m0.25\u001b[39;49m,\u001b[39m0.25\u001b[39;49m]\n\u001b[1;32m     27\u001b[0m                     [\u001b[39m0.25\u001b[39;49m, \u001b[39m0.25\u001b[39;49m, \u001b[39m0.35\u001b[39;49m,\u001b[39m0.15\u001b[39;49m]\n\u001b[1;32m     28\u001b[0m                     [\u001b[39m0.4\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.25\u001b[39m,\u001b[39m0.25\u001b[39m]\n\u001b[1;32m     29\u001b[0m                     [\u001b[39m0.05\u001b[39m, \u001b[39m0.45\u001b[39m, \u001b[39m0.25\u001b[39m,\u001b[39m0.25\u001b[39m]\n\u001b[1;32m     30\u001b[0m                     [\u001b[39m0.45\u001b[39m, \u001b[39m0.95\u001b[39m, \u001b[39m0.25\u001b[39m,\u001b[39m0.25\u001b[39m]\n\u001b[1;32m     31\u001b[0m                     [\u001b[39m0.25\u001b[39m, \u001b[39m0.25\u001b[39m, \u001b[39m0.45\u001b[39m,\u001b[39m0.05\u001b[39m]\n\u001b[1;32m     32\u001b[0m                     [\u001b[39m0.25\u001b[39m, \u001b[39m0.25\u001b[39m, \u001b[39m0.05\u001b[39m,\u001b[39m0.45\u001b[39m]])\n\u001b[1;32m     33\u001b[0m \u001b[39m# scX = preprocessing.StandardScaler()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# stY = preprocessing.StandardScaler()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[39m# X_train, X_validation, Y_train, Y_validation = data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m X_train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(X_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "def load_data(train_data):\n",
    "\n",
    "    train_ds = train_data\n",
    "    val_ds = leakage_val_1000\n",
    "\n",
    "    train_ds = train_ds.sample(frac=1)\n",
    "    val_ds = val_ds.sample(frac=1)\n",
    "\n",
    "    X_train = train_ds.iloc[:,2:].to_numpy()\n",
    "    Y_train = train_ds.iloc[:,:2]\n",
    "\n",
    "    X_validation = val_ds.iloc[:,2:].to_numpy()\n",
    "    Y_validation = val_ds.iloc[:,:2]\n",
    "\n",
    "    Y_train = Y_train.to_numpy()\n",
    "    Y_validation = Y_validation.to_numpy()\n",
    "    return X_train, Y_train, X_validation, Y_validation\n",
    "\n",
    "X_train, Y_train, X_validation, Y_validation = load_data(leakage_train_100)\n",
    "\n",
    "\n",
    "#test dataset\n",
    "X_test = np.array([[0.25, 0.25, 0.25,0.25], \n",
    "                    [0.35, 0.15, 0.25,0.25], \n",
    "                    [0.25, 0.25, 0.15,0.35],\n",
    "                    [0.15, 0.35, 0.25,0.25],\n",
    "                    [0.25, 0.25, 0.35,0.15],\n",
    "                    [0.4, 0.1, 0.25,0.25],\n",
    "                    [0.05, 0.45, 0.25,0.25],\n",
    "                    [0.45, 0.95, 0.25,0.25],\n",
    "                    [0.25, 0.25, 0.45,0.05],\n",
    "                    [0.25, 0.25, 0.05,0.45]])\n",
    "# scX = preprocessing.StandardScaler()\n",
    "# stY = preprocessing.StandardScaler()\n",
    "\n",
    "# def data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY):\n",
    "#     X_train = scX.fit_transform(X_train)\n",
    "#     X_validation = scX.transform(X_validation)\n",
    "\n",
    "#     Y_train = stY.fit_transform(Y_train)\n",
    "#     Y_validation = stY.transform(Y_validation)\n",
    "\n",
    "#     return X_train, X_validation, Y_train, Y_validation\n",
    "\n",
    "# X_train, X_validation, Y_train, Y_validation = data_scaling(X_train, X_validation, Y_train, Y_validation, scX, stY)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_validation = tf.convert_to_tensor(X_validation)\n",
    "Y_train = tf.convert_to_tensor(Y_train)\n",
    "Y_validation = tf.convert_to_tensor(Y_validation)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "\n",
    "num_rows, num_cols = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 1000\n",
    "# steps_per_epoch = sum(train_occurences) / batch_size\n",
    "starter_learning_rate = 1e-1\n",
    "end_learning_rate = 1e-8\n",
    "decay_steps = epochs * 3\n",
    "scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "    decay_steps= decay_steps,\n",
    "    end_learning_rate= end_learning_rate,\n",
    "    power=1)\n",
    "    \n",
    "def learning_curves(history)   :\n",
    "    sns.set_style('darkgrid', {'axes.facecolor': '.9'})\n",
    "    sns.set_context('notebook')\n",
    "\n",
    "    # your code\n",
    "    ### Learning curves\n",
    "    history_frame = pd.DataFrame(history.history)\n",
    "    history_frame.plot(figsize=(8, 5))\n",
    "    plt.show()\n",
    "\n",
    "def prediction_accuracy(predictions, Y_validation): \n",
    "    predictions = predictions.transpose()\n",
    "    Y_validation = tf.transpose(Y_validation)\n",
    "    y1 = predictions[0]\n",
    "    y2 = predictions[1]\n",
    "    y1_validation = Y_validation[0]\n",
    "    y2_validation = Y_validation[1]\n",
    "    fig, axs = plt.subplots(2)\n",
    "    # print(y1_validation.shape, y1.shape)\n",
    "    # print(y2_validation.shape, y2.shape)\n",
    "    # fig.suptitle('')\n",
    "    axs[0].scatter(y1_validation, y1)\n",
    "    axs[0].set_title('y1')\n",
    "    axs[1].scatter(y2_validation, y2)\n",
    "    axs[1].set_title('y2')\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='true value', ylabel='predicted value')\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    print(\"rmse of y1: \", mean_squared_error(y1_validation, y1, squared=False))\n",
    "    print(\"rmse of y2: \", mean_squared_error(y2_validation, y2, squared=False))\n",
    "\n",
    "# def dataloading(data):\n",
    "#     data = data.repeat()\n",
    "#     data = data.shuffle(buffer_size=1024, seed=0)\n",
    "#     data = data.batch(batch_size=batch_size)\n",
    "#     data = data.prefetch(buffer_size=1)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Augmentation\n",
    "# Requires cleaning up\n",
    "\n",
    "def rotation_matrix(angle):\n",
    "    theta = np.radians(angle)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c, -s), (s, c)))\n",
    "    return R\n",
    "\n",
    "def Augmentation_clock(x,y):\n",
    "\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    # print(y)\n",
    "    y_aug = np.transpose(np.matmul(rotation_matrix(-90), np.transpose(y)))\n",
    "    # print(y_aug)\n",
    "\n",
    "    temp = x.copy()\n",
    "    x0 = temp[:,0]\n",
    "    x1 = temp[:,1]\n",
    "    x2 = temp[:,2]\n",
    "    x3 = temp[:,3]\n",
    "\n",
    "    x[:,0] = x3\n",
    "    x[:,1] = x0\n",
    "    x[:,2] = x1\n",
    "    x[:,3] = x2\n",
    " \n",
    "    return x,y_aug\n",
    "\n",
    "def Augmentation_flip(x,y):\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    x = np.flip(x, axis=1)\n",
    "    y[:,1] = -1 * y[:,1]\n",
    "    return x,y\n",
    "\n",
    "def Augmentation_anticlock(x,y):\n",
    "\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    y_aug = np.transpose(np.matmul(rotation_matrix(90), np.transpose(y)))\n",
    "\n",
    "    temp = x.copy()\n",
    "    x0 = temp[:,0]\n",
    "    x1 = temp[:,1]\n",
    "    x2 = temp[:,2]\n",
    "    x3 = temp[:,3]\n",
    "\n",
    "    x[:,0] = x1\n",
    "    x[:,1] = x2\n",
    "    x[:,2] = x3\n",
    "    x[:,3] = x0\n",
    " \n",
    "    return x,y_aug\n",
    "\n",
    "def data_augmentation(x,y):\n",
    "    x = x.numpy()\n",
    "    y = y.numpy()\n",
    "    x_aug1,y_aug1 = Augmentation_clock(x, y)\n",
    "    x_aug2,y_aug2 = Augmentation_clock(x_aug1,y_aug1)\n",
    "    x_aug3,y_aug3 = Augmentation_clock(x_aug2,y_aug2)\n",
    "    x_aug4,y_aug4 = Augmentation_flip(x_aug3,y_aug3)\n",
    "    x_aug5,y_aug5 = Augmentation_clock(x_aug4,y_aug4)\n",
    "    x_aug6,y_aug6 = Augmentation_clock(x_aug5,y_aug5)\n",
    "    x_aug7,y_aug7 = Augmentation_clock(x_aug6,y_aug6)\n",
    "    X_train_Aug = np.concatenate((x, x_aug1, x_aug2, x_aug3, x_aug4, x_aug5, x_aug6, x_aug7))\n",
    "    Y_train_Aug = np.concatenate((y, y_aug1, y_aug2, y_aug3, y_aug4, y_aug5, y_aug6, y_aug7))\n",
    "\n",
    "    X_train_Aug = tf.convert_to_tensor(X_train_Aug)\n",
    "    Y_train_Aug = tf.convert_to_tensor(Y_train_Aug)\n",
    "\n",
    "    return X_train_Aug, Y_train_Aug\n",
    "\n",
    "X_train_Aug, Y_train_Aug = data_augmentation(X_train, Y_train)\n",
    "# X_validation_Aug, Y_validation_Aug = data_augmentation(X_validation, Y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network with subclassing api on Normal data\n",
    "\n",
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Hidden_layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],self.units), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],self.units), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = Hidden_layer(4)\n",
    "        self.dense2_1 = Output_layer(2)\n",
    "        self.relu = MyReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(x)\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = tf.keras.layers.Input(shape=(1,4))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "model = MyModel()\n",
    "model.build_graph().summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss='mean_squared_error',\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=50)],\n",
    "                    # shuffle=True\n",
    "                    )\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "prediction_accuracy(predictions, Y_validation)\n",
    "learning_curves(history)\n",
    "evaluate_train = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "\n",
    "evaluate_train = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "evaluate_validation = model.evaluate(X_validation,Y_validation, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network with subclassing api on Augmented data\n",
    "\n",
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Hidden_layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],self.units), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],self.units), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        return x\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = Hidden_layer(4)\n",
    "        self.dense2_1 = Output_layer(2)\n",
    "        self.relu = MyReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        return self.dense2_1(x)\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = tf.keras.layers.Input(shape=(1,4))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "model = MyModel()\n",
    "model.build_graph().summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss='mean_squared_error',\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train_Aug, Y_train_Aug, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=50)],\n",
    "                    # shuffle=True\n",
    "                    )\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "prediction_accuracy(predictions, Y_validation)\n",
    "learning_curves(history)\n",
    "\n",
    "evaluate_train = model.evaluate(X_train_Aug, Y_train_Aug, batch_size=batch_size)\n",
    "evaluate_validation = model.evaluate(X_validation,Y_validation, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivariant NN - on Normal data\n",
    "\n",
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Hidden_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.W = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.a = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.b = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.c = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.a_matrix = tf.constant([[1,0,0,0], [0,1,0,0], [0,0,1,0],[0,0,0,1]], dtype=tf.float64)\n",
    "        self.b_matrix = tf.constant([[0,1,0,1], [1,0,1,0], [0,1,0,1],[1,0,1,0]], dtype=tf.float64)\n",
    "        self.c_matrix = tf.constant([[0,0,1,0], [0,0,0,1], [1,0,0,0],[0,1,0,0]], dtype=tf.float64)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.W = tf.multiply(self.a, self.a_matrix) + tf.multiply(self.b, self.b_matrix) + tf.multiply(self.c, self.c_matrix)\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        # tf.print(self.W)\n",
    "        return x\n",
    "\n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.d_matrix = tf.constant([[1,-1], [-1,-1], [-1,1],[1,1]], dtype=tf.float64)\n",
    "        # self.d_matrix = tf.constant([[1,-1, -1,1], [-1,-1,1,1]], dtype=tf.float64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.W = tf.multiply(self.d, self.d_matrix)\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        # tf.print(tf.transpose(self.W))\n",
    "        return x\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = Hidden_layer(4)\n",
    "        self.dense2_1 = Output_layer(2)\n",
    "        # self.dense2_2 = Output_layer(1)\n",
    "        self.relu = MyReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "        return self.dense2_1(x)\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = tf.keras.layers.Input(shape=(1,4))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "model = MyModel()\n",
    "model.build_graph().summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss='mean_squared_error',\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=50)],\n",
    "                    # shuffle=True\n",
    "                    )\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "prediction_accuracy(predictions, Y_validation)\n",
    "learning_curves(history)\n",
    "\n",
    "evaluate_train = model.evaluate(X_train, Y_train, batch_size=batch_size)\n",
    "evaluate_validation = model.evaluate(X_validation,Y_validation, batch_size=batch_size)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivariant NN - on Normal data\n",
    "\n",
    "class Hidden_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Hidden_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.W = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.a = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.b = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.c = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.a_matrix = tf.constant([[1,0,0,0], [0,1,0,0], [0,0,1,0],[0,0,0,1]], dtype=tf.float64)\n",
    "        self.b_matrix = tf.constant([[0,1,0,1], [1,0,1,0], [0,1,0,1],[1,0,1,0]], dtype=tf.float64)\n",
    "        self.c_matrix = tf.constant([[0,0,1,0], [0,0,0,1], [1,0,0,0],[0,1,0,0]], dtype=tf.float64)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.W = tf.multiply(self.a, self.a_matrix) + tf.multiply(self.b, self.b_matrix) + tf.multiply(self.c, self.c_matrix)\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        # tf.print(self.W)\n",
    "        return x\n",
    "\n",
    "class Output_layer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d = self.add_weight(shape=(1,), initializer=tf.keras.initializers.HeUniform, trainable=True)\n",
    "        self.d_matrix = tf.constant([[1,-1], [-1,-1], [-1,1],[1,1]], dtype=tf.float64)\n",
    "        # self.d_matrix = tf.constant([[1,-1, -1,1], [-1,-1,1,1]], dtype=tf.float64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.W = tf.multiply(self.d, self.d_matrix)\n",
    "        x = tf.matmul(inputs, self.W)\n",
    "        # tf.print(tf.transpose(self.W))\n",
    "        return x\n",
    "\n",
    "class MyReLU(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyReLU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = Hidden_layer(4)\n",
    "        self.dense2_1 = Output_layer(2)\n",
    "        # self.dense2_2 = Output_layer(1)\n",
    "        self.relu = MyReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "        return self.dense2_1(x)\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = tf.keras.layers.Input(shape=(1,4))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "model = MyModel()\n",
    "model.build_graph().summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scheduler),\n",
    "              loss='mean_squared_error',\n",
    "              metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train_Aug, Y_train_Aug, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=50)],\n",
    "                    # shuffle=True\n",
    "                    )\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "prediction_accuracy(predictions, Y_validation)\n",
    "learning_curves(history)\n",
    "\n",
    "evaluate_train = model.evaluate(X_train_Aug, Y_train_Aug, batch_size=batch_size)\n",
    "evaluate_validation = model.evaluate(X_validation,Y_validation, batch_size=batch_size)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Equivariant NN - by averaging weights (doesnt work) and Kernel constraint\n",
    "# # how to make constraint work on subclassing api\n",
    "# # Equivariance NN with subclassing API - experiment with layer constraint\n",
    "# # These function make use of the subclassing Api and custom layer functionality of tensorflow\n",
    "# # experiment\n",
    "\n",
    "# class Hidden_layer(layers.Layer):\n",
    "#     def __init__(self, units, kernel_constraint):\n",
    "#         super(Hidden_layer, self).__init__()\n",
    "#         self.units = units\n",
    "#         self.kernel_constraint = kernel_constraint\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[-1], self.units),\n",
    "#             # initializer=tf.keras.initializers.GlorotNormal, \n",
    "#             initializer=tf.keras.initializers.HeUniform,\n",
    "#             trainable=True, constraint=self.kernel_constraint) \n",
    "            \n",
    "#     def call(self, inputs):\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         return x\n",
    "\n",
    "# class Hidden_layer_constraint(tf.keras.constraints.Constraint):\n",
    "#     # def __init__(self):\n",
    "#     #     self.d = None\n",
    "\n",
    "#     # @tf.function\n",
    "#     def __call__(self, w):\n",
    "#         a = tf.reduce_mean([w[0,0], w[1,1], w[2,2], w[3,3]])\n",
    "#         b = tf.reduce_mean([w[0,1], w[1,0], w[1,2], w[2,1], w[0,3], w[3,0], w[2,3], w[3,2]])\n",
    "#         c = tf.reduce_mean([w[0,2], w[1,3], w[2,0], w[3,1]])\n",
    "        \n",
    "#         row_indices = tf.constant([0, 1, 2, 3])\n",
    "#         col_indices = tf.constant([0, 1, 2, 3])\n",
    "#         w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(a, tf.shape(row_indices)[0]))\n",
    "\n",
    "#         row_indices = tf.constant([0, 0, 1, 1, 2, 2, 3, 3])\n",
    "#         col_indices = tf.constant([1, 3, 2, 0, 1, 3, 0, 2])\n",
    "#         w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(b, tf.shape(row_indices)[0]))\n",
    "\n",
    "#         row_indices = tf.constant([0, 1, 3, 2])\n",
    "#         col_indices = tf.constant([2, 3, 1, 0])\n",
    "#         w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(c, tf.shape(row_indices)[0]))\n",
    "\n",
    "#         return w\n",
    "\n",
    "# class Output_layer_constraint(tf.keras.constraints.Constraint):\n",
    "#     # def __init__(self):\n",
    "#     #     self.d = None\n",
    "\n",
    "#     # @tf.function\n",
    "#     def __call__(self, w):\n",
    "#         self.d = tf.reduce_mean(w)\n",
    "#         w = w/w * self.d\n",
    "#         row_indices = tf.constant([0, 1, 1, 2])\n",
    "#         col_indices = tf.constant([1, 0, 1, 0])\n",
    "#         w = tf.tensor_scatter_nd_update(w, tf.stack([row_indices, col_indices], axis=1), tf.repeat(-self.d, tf.shape(row_indices)[0]))\n",
    "#         return w\n",
    "\n",
    "# class Output_layer(layers.Layer):\n",
    "#     def __init__(self, units, kernel_constraint):\n",
    "#         super(Output_layer, self).__init__()\n",
    "#         self.units = units\n",
    "#         self.kernel_constraint = kernel_constraint\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[-1], self.units),\n",
    "#             # initializer=tf.keras.initializers.GlorotNormal, \n",
    "#             initializer=tf.keras.initializers.HeUniform,\n",
    "#             trainable=True, constraint=self.kernel_constraint\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         return x\n",
    "\n",
    "# class MyReLU(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(MyReLU, self).__init__()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return tf.math.maximum(x, 0)\n",
    "\n",
    "# class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self). __init__()\n",
    "#         self.dense1 = Hidden_layer(4, kernel_constraint=Hidden_layer_constraint())\n",
    "#         self.dense2 = Hidden_layer(4, kernel_constraint=Hidden_layer_constraint())\n",
    "#         # self.dense2_1 = tf.keras.layers.Dense(2, kernel_constraint=Output_layer_constraint())\n",
    "#         self.dense3_1 = Output_layer(2, kernel_constraint=Output_layer_constraint())\n",
    "#         self.relu = MyReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.relu(self.dense1(x))\n",
    "#         x = self.relu(self.dense2(x))\n",
    "#         # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "#         return self.dense3_1(x)\n",
    "\n",
    "#     def build_graph(self):\n",
    "#         x = tf.keras.layers.Input(shape=(1,4))\n",
    "#         return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "# model = MyModel()\n",
    "# model.build_graph().summary()\n",
    "\n",
    "# starter_learning_rate = 1e-0\n",
    "# end_learning_rate = 1e-7\n",
    "# decay_steps = 100\n",
    "# scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate= starter_learning_rate,\n",
    "#     decay_steps= decay_steps,\n",
    "#     end_learning_rate= end_learning_rate,\n",
    "#     power=1)\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "#               loss=tf.keras.losses.mean_squared_error,\n",
    "#               metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "# )\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=epochs, batch_size= batch_size, verbose=2, validation_data=(X_validation, Y_validation),\n",
    "#                     # callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    "#                     )\n",
    "\n",
    "# print(\"output layer weight matrix\")\n",
    "# print(model.layers[2].weights)\n",
    "\n",
    "# print(\"Hidden layer weight matrix\")\n",
    "# print(model.layers[1].weights)\n",
    "\n",
    "# print(\"Hidden layer weight matrix\")\n",
    "# print(model.layers[0].weights)\n",
    "\n",
    "# predictions = model.predict(X_validation)\n",
    "# prediction_accuracy(predictions, Y_validation)\n",
    "# learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Functional Tuning - Option 2: using Keras Tuner ------------------------------\n",
    "# # Goal: tune the learning rate\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# # 1. Define the general architecture of the model through a creation user-defined function\n",
    "# def model_builder(hp):\n",
    "#   model = Sequential()\n",
    "#   model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n",
    "#   model.add(Dense(8, activation='relu'))\n",
    "#   model.add(Dense(1))\n",
    "#   hp_learning_rate = hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3, 1e-4]) # Tuning the learning rate (four different values to test: 0.1, 0.01, 0.001, 0.0001)\n",
    "#   #optimizer = RMSprop(learning_rate = hp_learning_rate) \n",
    "#   optimizer = tf.keras.optimizers.Adam(learning_rate = hp_learning_rate) # Defining the optimizer\n",
    "#   model.compile(loss='mse',metrics=['mse'], optimizer=optimizer)                   # Compiling the model \n",
    "#   return model                                                                     # Returning the defined model\n",
    "\n",
    "# # 2. Define the hyperparameters grid to be validated\n",
    "# tuner_rs = kt.RandomSearch(\n",
    "#               model_builder,                # Takes hyperparameters (hp) and returns a Model instance\n",
    "#               objective = 'mse',            # Name of model metric to minimize or maximize\n",
    "#               seed = 42,                    # Random seed for replication purposes\n",
    "#               max_trials = 5,               # Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested.\n",
    "#               directory='random_search')    # Path to the working directory (relative).\n",
    "\n",
    "# # 3. Run the GridSearchCV process\n",
    "# tuner_rs.search(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not important Experiment 2 - learned how to make a neural network with subclassing api\n",
    "# # These function make use of the subclassing Api and custom layer functionality of tensorflow\n",
    "# # experiment\n",
    "\n",
    "# class Hidden_layer(layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Hidden_layer, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.a = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         self.b = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         self.c = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         x= tf.Variable(\n",
    "#             [self.a, self.b, self.c, self.b, \n",
    "#              self.b, self.a, self.b, self.c, \n",
    "#              self.c, self.b, self.a, self.b, \n",
    "#              self.b, self.c, self.b, self.a])\n",
    "#         x = tf.reshape(x, shape=(input_shape[-1],self.units))\n",
    "#         self.W = tf.Variable(x, shape=(input_shape[-1],self.units),trainable=True)\n",
    "#     def call(self, inputs):\n",
    "#         # print(self.W.shape)\n",
    "#         # print(inputs.shape)\n",
    "#         # # print(\"a\")\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         # print(\"b\")\n",
    "#         # print(x)\n",
    "#         # print(x.shape)\n",
    "#         return x\n",
    "\n",
    "# class Output_layer(layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Output_layer, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.d = self.add_weight(shape=(1,), initializer=tf.keras.initializers.GlorotNormal, trainable=False)\n",
    "#         x = tf.Variable([self.d,-self.d,-self.d,self.d, -self.d,-self.d,self.d,self.d])\n",
    "#         x = r = tf.reshape(x, shape=(input_shape[-1],self.units))\n",
    "#         self.W = tf.Variable(x, shape=(input_shape[-1],self.units),trainable=True)\n",
    "        \n",
    "#         # self.W = tf.Variable(\n",
    "#         #     [[self.d,-self.d],\n",
    "#         #     [-self.d,-self.d], \n",
    "#         #      [-self.d,-self.d],\n",
    "#         #      [self.d,self.d]],shape=(input_shape[-1],self.units),\n",
    "#         #     trainable=True)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # inputs = inputs\n",
    "#         # W = self.W.transpose()\n",
    "#         # print(self.W.shape)\n",
    "#         # print(inputs.shape)\n",
    "#         # print(\"a\")\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         # print(\"b\")\n",
    "#         # print(x.shape)\n",
    "#         return x\n",
    "\n",
    "# class MyReLU(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(MyReLU, self).__init__()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return tf.math.maximum(x, 0)\n",
    "\n",
    "# class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.dense1 = Hidden_layer(4)\n",
    "#         self.dense2_1 = Output_layer(2)\n",
    "#         # self.dense2_2 = Output_layer(1)\n",
    "#         self.relu = MyReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.relu(self.dense1(x))\n",
    "#         # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "#         return self.dense2_1(x)\n",
    "\n",
    "#     def build_graph(self):\n",
    "#         x = tf.keras.layers.Input(shape=(1,4))\n",
    "#         return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "# model = MyModel()\n",
    "# model.build_graph().summary()\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y=Y_train, epochs=5, batch_size=5, verbose=2)\n",
    "\n",
    "# print(\"output layer weight\")\n",
    "# print(model.layers[1].weights)\n",
    "\n",
    "# print(\"hidden layer weight\")\n",
    "# print(model.layers[0].weights)\n",
    "\n",
    "# predictions = model.predict(X_validation)\n",
    "# prediction_accuracy(predictions, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not important Experiment 3 - learned how to give custom kernel constraint\n",
    "# # how to make constraint work on subclassing api\n",
    "# # Equivariance NN with subclassing API - experiment with layer constraint\n",
    "# # These function make use of the subclassing Api and custom layer functionality of tensorflow\n",
    "# # experiment\n",
    "\n",
    "# class Hidden_layer(layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Hidden_layer, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[-1], self.units),initializer=tf.keras.initializers.GlorotNormal, trainable=True)\n",
    "            \n",
    "#     def call(self, inputs):\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         return x\n",
    "\n",
    "# class Output_layer_constraint(tf.keras.constraints.Constraint):\n",
    "#     # def __call__(self, d):\n",
    "#     #     self.d = d\n",
    "#     def __call__(self, w):\n",
    "#         # x = tf.Variable([self.d,-self.d,-self.d,self.d, -self.d,-self.d,self.d,self.d])\n",
    "#         # w = tf.reshape(x, shape=(4.4))\n",
    "    \n",
    "#         return w/w\n",
    "\n",
    "# class Output_layer(layers.Layer):\n",
    "#     def __init__(self, units, kernel_constraint):\n",
    "#         super(Output_layer, self).__init__()\n",
    "#         self.units = units\n",
    "#         self.kernel_constraint = kernel_constraint\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[-1], self.units),initializer=tf.keras.initializers.GlorotNormal, trainable=True, constraint=self.kernel_constraint\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = tf.matmul(inputs, self.W)\n",
    "#         return x\n",
    "\n",
    "# class MyReLU(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(MyReLU, self).__init__()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return tf.math.maximum(x, 0)\n",
    "\n",
    "# class MyModel(keras.Model):  # model.fit, model.evalute, model.predict\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.dense1 = Hidden_layer(4)\n",
    "#         # self.dense2_1 = tf.keras.layers.Dense(2, kernel_constraint=Output_layer_constraint())\n",
    "#         self.dense2_1 = Output_layer(2, kernel_constraint=Output_layer_constraint())\n",
    "#         self.relu = MyReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.relu(self.dense1(x))\n",
    "#         # return {\"y1\" :self.dense2_1(x),\"y2\" : self.dense2_2(x)}\n",
    "#         return self.dense2_1(x)\n",
    "\n",
    "#     # def build_graph(self):\n",
    "#     #     x = tf.keras.layers.Input(shape=(1,4))\n",
    "#     #     return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "# model = MyModel()\n",
    "# # model.build_graph().summary()\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y=Y_train, epochs=5, batch_size=5, verbose=2)\n",
    "# print(model.layers[1].weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7e9f4964f105db5d476a6bf9b47e70d0bf16854eb8b7ea4fd7f47088c3e55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
